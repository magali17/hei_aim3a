---
title: "Temporal adjustment"
author: "Magali Blanco"
date: ' `r Sys.time()` '
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
    number_sections: true
    toc_float: true
    collapsed: false
    smooth_scroll: false
editor_options: 
  chunk_output_type: console
---


```{r, echo=F, warning=F}
##################################################################################################
# SETUP
##################################################################################################
knitr::opts_chunk$set(echo = F, 
                      cache=F, cache.comments = F, 
                      message = F, warning = F#, tidy.opts=list(width.cutoff=60), tidy=TRUE 
                      )  

# Clear workspace of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
  res <- suppressWarnings(
    lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
           detach, character.only=TRUE, unload=TRUE, force=TRUE))
}

pacman::p_load(tidyverse, 
               # ggpmisc, #stat_poly_eq()
               # parallel, #mclapply; detectCores()
               # future.apply, #future_replicate()
               lubridate, # %within%
               splines, #bs() in lm()
               kableExtra,
               mgcv#, # gam() #s(, bs="cc") # for cyclic spline
               #zoo #na.approx(); rolling mean
)    

source("functions.R")
set.seed(1)

## for future.apply::future_replicate()  
# availableCores() #8 
#plan(multisession, workers = 6)

latest_version <- "v3_20230321" 
dt_path <- file.path("Output", latest_version)

local_tz <- "America/Los_Angeles"
bdate <-ymd("2019-02-22")
edate <-ymd("2020-03-17") 

image_path <- file.path("..", "Manuscript", "Images", "v4", "SI")
if(!dir.exists(image_path)) {dir.create(image_path, recursive = T)}

if(!dir.exists(file.path(dt_path, "campaign visit samples"))) {dir.create(file.path(dt_path, "campaign visit samples"))}

# ggplot settings
theme_set(theme_bw())
theme_update(legend.position = "bottom")

##################################################################################################
# LOAD DATA
##################################################################################################
# NO2 data is avaialble at 2 sites, whereas BC data is only available at 10W.  
# only hourly (not minute-level) data are available through the PSCAA Air Data website (https://secure.pscleanair.org/airgraphing)
# 2019 report: https://pscleanair.gov/DocumentCenter/View/4164/Air-Quality-Data-Summary-2019 
# study dates: "2019-02-22" "2020-03-17"

nox <- readRDS(file.path("data", "epa_data_mart", "wa_county_nox.rda")) %>%  
  mutate(
    # NOTE: daylight savings (3/10/2019 & 3/10/2020 @ 2 AM) get dropped. why are these readings included in this dataset? are they duplicates? 
    time = paste(date_local, time_local),
    time = ymd_hm(time, tz=local_tz)) %>%
  filter(date_local >= bdate & date_local <= edate,
    grepl("NO2", parameter),
     site_name %in% c("Beacon Hill")) %>%
  drop_na(time) %>%
  select(site_name, time, date=date_local, time_local, no2=sample_measurement, units_of_measure) %>%
  
  add_season(.date_var = "date") %>% 
  mutate(month = month(date, label=TRUE),
         month = factor(month, ordered = F),
         dow = wday(date, label=T, week_start = "Monday"),
         dow = factor(dow, ordered = F),
         hour = hour(time),
         hour = factor(hour)
         ) %>%
  arrange(time)

# Overnight data 
## NS is 1-minute data; P-trak is 10-sec data originally
overnight <- readRDS(file.path("data", "Overnight Collocations", "overnight_data_2023-09-11.rda")) %>%
  #most of the data is at BH
  filter(
    # 10W data is only for March 2020; WILCOX data is for after the study period
    site == "AQSBH",
    # variable == "ns_total_conc",
    # # most (86.5%) of the data comes from PMSCAN_3 (vs PMSCAN_2)
    # # drop PMSCAN_2. It is used less & produces higher readings when collocated in June that would probably need to be calibrated 
    # instrument_id=="PMSCAN_3"
    instrument_id !="PMSCAN_2"
    ) %>%
  rename(time=timeint) %>%
  mutate(date = date(time),
         month = month(time, label=TRUE),
         month = factor(month, ordered = F),
         dow = wday(time, label=T, week_start = "Monday"),
         dow = factor(dow, ordered = F),
         weekday = ifelse(dow %in% c("Sat", "Sun"), "weekend", "weekday"),
         hour = hour(time),
         hour = factor(hour),
         ) %>%  
  # hourly averages
  # note that there are 2 Nanoscans (1 collocated period in July) & 2 P-traks that sample at different times
  group_by(site, date, hour, month, dow, weekday, variable) %>%
  summarize(value = mean(value, na.rm = T)) %>%
  ungroup()


# ggplot(data=overnight, aes(y=instrument_id, x=time)) + geom_point()

# # note that there are 2 Nanoscans (1 collocated period in July) & 2 P-traks that sample at different times
# test <- overnight %>% 
#   filter(instrument_id %in% c("PMSCAN_2", "PMSCAN_3")) %>% 
#   pivot_wider(names_from = instrument_id, values_from = value) %>%
#   drop_na(contains("PMSCAN_"))
# cor(test$PMSCAN_2, test$PMSCAN_3)
  
saveRDS(overnight, file.path("data", "Overnight Collocations", #"overnight_data_2023-09-11_modified.rda"
                             "overnight_data_2024-05-15.rda"
                             ))

unbalanced_samples0 <- readRDS(file.path(dt_path, "campaign visit samples", "fewer_hours.rda")) %>%
  select(design, version, campaign, location, time, dow=day, hour, season, weekday=tow2, no2, pnc_noscreen, ns_total_conc) %>%
  mutate(hour = factor(hour, levels = levels(overnight$hour)),
         dow = factor(dow, levels = levels(overnight$dow), ordered = F))


# all data (e.g., for correlations)
stops_w <- readRDS(file.path(dt_path, "stops_used.rda"))

# all-data estimates
all_data_annual <- stops_w %>%
  group_by(location) %>%
  summarize(ns_total_conc_all_data = mean(ns_total_conc))

########################################
# from r0_temporal_adjustment.R
## using the approach w/o highway data
main_bg <- "hr3_pct1"

# # --> CHECK THAT TZ = LA/AMERICAL
# temp_adj2 <- readRDS(file.path("data", "onroad", "annie", "v2", "temporal_adj", 
#                                "20240421", # CHECK THAT THIS IS THE LATEST VERSION
#                                "underwrite_temp_adj_no_hwy.rda")) %>%
#   filter(background_adj == main_bg) %>%
#   select(time, avg_hourly_adj)
 
```

# NS vs Ptrak data

```{r}
cor_dt <- overnight %>%
  pivot_wider(names_from = variable, values_from = value) %>%
  drop_na(ns_total_conc, pnc_noscreen) 

# number of paired hour observations
nrow(cor_dt) # 342 hour pairs

print("correlation between hourly avg P-TRAK and NS measures")
cor(cor_dt$ns_total_conc, cor_dt$pnc_noscreen, use="complete.obs")


```


# ADJUSTMENT 1
## check regulatory data

```{r}
# total number of samples
nox %>%
  #drop previously missing values fro counts
  drop_na(no2) %>%
  
  group_by(site_name) %>%
  summarize(
    start_date = min(date),
    end_date = max(date),
    hours_in_sampling_period = interval(bdate, edate)/days(1)*24,
    hour_samples = n(),
    prop_hours = hour_samples/hours_in_sampling_period
  ) %>%
  kableExtra::kable(digits = 3) %>%
  kableExtra::kable_styling()


print("proportion of missing values (hours)")
prop.table(table(is.na(nox$no2))) %>% round(2)

# # amount of data available
# nox %>%
# 
#   #filter(no2_interpolated==FALSE) %>%
# 
#   ggplot(aes(x=hour, y=dow)) +
#   geom_bin_2d() +
#   labs(title = "Number of readings (i.e., weekly readings) available during the study period")

missing_hours <- nox %>%
  filter(is.na(no2)) %>%
  group_by(date) %>%
  summarize(n = n())

print("missing hours per day in days with missingness. most days with missingness have 1-2 missing hours")
table(missing_hours$n)

```

### Interpolate missing NO2 values

interpolate missing NO2 using a fitted model (vs simple linear interpolation) since 3 days have 16, 22, and 24 missing hourly values

```{r}
missing_no2_lm <- nox %>%
  mutate(hour = as.numeric(as.character(hour))) %>%
  gam(data=., formula = log(no2) ~ month + 
        dow +  #new 12/20/23
        s(hour,
          bs="cc", #cyclic cubic regression spline
          k=6, # (later, I use 5, but this seems to increase performance by a larger bump than 4 to 5, little added in-sample performance is seen w/ more flexibility
          by = dow #interaction by dow
          )) 


summary(missing_no2_lm)

print("the hour functions look non-linear")
par(mfrow = c(3, 3))
plot(missing_no2_lm)
par(mfrow = c(1, 1))


predicted_no2 <- predict(missing_no2_lm, 
                         newdata = nox %>% mutate(hour = as.numeric(as.character(hour)))) %>% 
  exp()

nox <- nox %>%
  mutate(no2_interpolated = ifelse(is.na(no2), TRUE, FALSE),
         predicted_no2 = predicted_no2,
         no2 = ifelse(is.na(no2), predicted_no2, no2)) %>% 
  select(-predicted_no2)

```

## summary of overnight UFP data

```{r, fig.width=8}
## nanoscan
overnight %>%
  #filter(month == first(month)) %>%
  mutate(time = ymd_h(paste(date, hour), tz=local_tz)) %>% 
  ggplot(aes(x=time, y=value, )) + 
  facet_wrap(~variable) +
  geom_point(alpha=0.4, aes(col=month)) + 
  geom_smooth() + 
  labs(title = "Overnight NS data")

# available data
overnight %>%
  group_by(month, variable) %>%
  summarize(dates = length(unique(date)),
            start = min(date),
            end = max(date),
            #seasons = paste(unique(season), collapse = ", "),
            days = paste(unique(dow), collapse = ", "),
            ) %>%
  arrange(variable)

#boxplots of distributions
overnight %>%
  filter(site == "AQSBH") %>%
  pivot_longer(cols = c(month, dow, hour), names_to = "time_var", values_to = "time_value") %>% 
  mutate(time_var = ifelse(time_var=="dow", "day", time_var)) %>%  
  
  group_by(time_value, time_var, variable) %>%
  summary_table(var = "value") %>%
  
  filter(variable=="ns_total_conc") %>%
  
  ggplot(aes(x=time_value)) +
  geom_boxplot(stat="identity", aes(ymin=Q10, lower=Q25, middle=Q50, upper=Q75, ymax=Q90)) +
  #facet_wrap(~time_var+variable, scales="free", switch = "x", ncol = 2) +
  facet_wrap(~time_var, scales="free", switch = "x", ncol = 2) +
  #geom_boxplot() +
  labs(x = "Time", y="PNC (pt/cm3)")

ggsave(file.path(image_path, "overnight_pnc_beaconhill.png"), width = 9, height = 7)

```

## NO2 vs UFP

```{r}
# using overnight & regulatory data
winsorize_quantile <- 0.05

# combine nox & ufp data by time
calibration_dt <- overnight %>%
  filter(site == "AQSBH") %>%
  pivot_wider(names_from = variable,values_from = value) %>%
  select(date, month, dow, hour, ns_total_conc, pnc_noscreen) %>% 
  # note that this changes the date to ??UTC? the
  #add_season(.date_var = "date") %>% 
  left_join(nox %>%  
              select(date, hour, no2, season)) %>%
  drop_na() 

# time series 
ufp_conversion <- 1e3

calibration_dt %>%
  mutate(ns_total_conc = ns_total_conc/ufp_conversion) %>%
  pivot_longer(cols = c(ns_total_conc, contains("no2")), names_to = "pollutant") %>%
  mutate(time = ymd_h(paste(date, hour), tz=local_tz),
         pollutant = ifelse(pollutant == "no2", "NO2 (ppb)",
                            ifelse(pollutant == "ns_total_conc", "PNC (1,000 pt/cm3)", NA))
         ) %>%
   
  
  
  ggplot(aes(x=time, y=value, col=pollutant)) + 
  facet_wrap(~month, scales = "free") + 
  geom_point(alpha=0.2) + 
  # make smooth line more wiggly to show hourly patterns (what we are interested in)
  geom_smooth(span=0.05, #0.1
              se = F) + 
  labs(#title = paste0("time series of hourly NO2 (ppb) and UFP (", ufp_conversion, " pt/cm3) at Beacon Hill"), 
       #subtitle = "Overnight data",
       y="Observed Concentration",
       col="Pollutant")

ggsave(file.path(image_path, "no2_pnc_time_trend.png"), width = 10, height = 6)
  
# --> same as above for p-trak
calibration_dt %>%
  mutate(pnc_noscreen = pnc_noscreen/ufp_conversion) %>%
  pivot_longer(cols = c(pnc_noscreen, contains("no2")),names_to = "pollutant") %>%
  mutate(time = ymd_h(paste(date, hour), tz=local_tz)) %>%
  
  ggplot(aes(x=time, y=value, col=pollutant)) + 
  facet_wrap(~month, scales = "free") + 
  geom_point(alpha=0.2) + 
  # make smooth line more wiggly to show hourly patterns (what we are interested in)
  geom_smooth(span=0.1, se = F) + 
  labs(#title = paste0("time series of hourly NO2 (ppb) and UFP (", ufp_conversion, " pt/cm3) at Beacon Hill"), 
       #subtitle = "Overnight data",
       y="Observed Concentration",
       col="Pollutant")

ggsave(file.path(image_path, "no2_pnc_time_trend_ptrak.png"), width = 8, height = 6)


```

 

### UFP-NO2 prediction model

leverage NO2 to capture the monthly/seasonal patterns not otherwise captured by limited UFP data alone

#### model 

why does residuals(lm1) %>% exp() produce different results? has to do with residuals being in the log scale initially? 

```{r}
# for K knots, we fit K+1 cubic polynomials
# a cubic spline with K nots fits 4+k df [or 6 if no intercept?]


#  Documentation says that for factors, a main effect for dow should probably been included (this is automatically generated like with lm()).  
print("fitting cubic cyclic spline")
set.seed(2)
# use cyclic spline?? mgcg::s(x, bs = "ad", xt = list(bs = "cc")) 
lm1 <-  calibration_dt %>%
  mutate(hour = as.numeric(as.character(hour))) %>%
  gam(data=., formula=log(ns_total_conc)~log(no2) + dow + 
                                                    s(hour,
                                                      bs="cc", #cyclic cubic regression spline
                                                      k=5, #k=5 or 6? both perform similarly
                                                      by = dow #interaction by dow
                                                      )) 

summary(lm1)

print("the hour functions look non-linear")
par(mfrow = c(3, 3))
plot(lm1)

#same as above for p-trak
set.seed(2)
# use cyclic spline?? mgcg::s(x, bs = "ad", xt = list(bs = "cc")) 
lm1_ptrak <-  calibration_dt %>%
  mutate(hour = as.numeric(as.character(hour))) %>%
  gam(data=., formula=log(pnc_noscreen)~log(no2) + dow + 
                                                    s(hour,
                                                      bs="cc", #cyclic cubic regression spline
                                                      k=5, #k=5 or 6? both perform similarly
                                                      by = dow #interaction by dow
                                                      )) 

summary(lm1_ptrak)


calibration_dt <- calibration_dt %>%
  mutate(ufp_prediction = predict(lm1) %>% exp(),
         ufp_prediction_ptrak = predict(lm1_ptrak) %>% exp(),
         residual = pnc_noscreen - ufp_prediction_ptrak,
         residual_ptrak = ns_total_conc - ufp_prediction,
         time = ymd_h(paste(date, hour),  tz=local_tz)) 

```

```{r}
# compare UFP measurements (31 dys) to predictions
# calibration_dt %>%
#   select(date, hour, ns_total_conc, contains("ufp_prediction")) %>%
#   pivot_longer(cols = contains("ufp_prediction")) %>%
#   mutate(name = ifelse(name=="ufp_prediction", "current UFP model", "model with DOW main effect")) %>%
#   
#   ggplot(aes(x=ns_total_conc, y=value, col=name)) + 
#   geom_point() + 
#   geom_smooth() + 
#   geom_abline(slope = 1, intercept = 0, linetype=2) +
#   theme(aspect.ratio = 1) +
#   labs(x="Observed UFP",
#        y = "Predicted UFP",
#        col=NULL,
#        title = "Hourly UFP (pt/cm3) at Beacon Hill (31 dys)\nduring the study period"
#        )
# #ggsave(file.path(image_path, "..", "other", "qc", "predicted_ufp_vs_measured_dow_main_effect.png"), width = 6, height = 6)


```


#### residuals - is the temporal pattern gone? 

i.e., does NO2 capture UFP's temporal trend? 

mostly? 

```{r}
y_breaks <- c(c(0, 3, 10, 20)*1e3,
                 c(3, 10, 20)*-1e3)

# time series 
## NS
calibration_dt %>%
  ggplot(aes(x=time, y=residual)) + 
  facet_wrap(~month, scales = "free_x") + 
  geom_hline(yintercept = 0, linetype=2, alpha=0.6) + 
  geom_hline(yintercept = c(-3e3, 3e3), linetype=3, alpha=0.5) + 
  geom_point(aes(col=dow), alpha=0.4) + 
  geom_smooth(#span=0.3
              ) +
  scale_y_continuous(breaks = y_breaks) +
  labs(#title = paste0("time series of UFP residuals from the model (native scale)")
    )

ggsave(file.path(image_path, "no2_pnc_residuals_time_trend.png"), width = 8, height = 6)

print("distribution of residuals")
summary(calibration_dt$residual) 


## ptrak
calibration_dt %>%
  ggplot(aes(x=time, y=residual_ptrak)) + 
  facet_wrap(~month, scales = "free_x") + 
  geom_hline(yintercept = 0, linetype=2, alpha=0.6) + 
  geom_hline(yintercept = c(-3e3, 3e3), linetype=3, alpha=0.5) + 
  geom_point(aes(col=dow), alpha=0.4) + 
  geom_smooth(#span=0.3
              ) +
  scale_y_continuous(breaks = y_breaks) +
  labs(#title = paste0("time series of UFP residuals from the model (native scale)")
    )

ggsave(file.path(image_path, "no2_pnc_residuals_time_trend_ptrak.png"), width = 8, height = 6)

```

```{r}
# # remove alternative prediction
# calibration_dt <- calibration_dt %>%
#   select(-ufp_prediction_alt)

```

#### out-of-sample model performance

```{r}

```

#### generate synthetic data & temporal adjustments

```{r}
predicted_ufp <- nox %>%
  mutate(hour = as.numeric(as.character(hour))) %>% 
  predict(lm1, newdata=.) %>% 
  exp()

predicted_ufp_ptrak <- nox %>%
  mutate(hour = as.numeric(as.character(hour))) %>% 
  predict(lm1_ptrak, newdata=.) %>% 
  exp()

predicted_lta_ufp <- mean(predicted_ufp)
predicted_lta_ufp_ptrak <- mean(predicted_ufp_ptrak)

predicted_ufp_low <- quantile(predicted_ufp, winsorize_quantile, na.rm = T)
predicted_ufp_high <- quantile(predicted_ufp, 1-winsorize_quantile, na.rm = T)

predicted_ufp_low_ptrak <- quantile(predicted_ufp_ptrak, winsorize_quantile, na.rm = T)
predicted_ufp_high_ptrak <- quantile(predicted_ufp_ptrak, 1-winsorize_quantile, na.rm = T)


nox <- nox %>%
  mutate(predicted_ufp = predicted_ufp,
         predicted_ufp_ptrak = predicted_ufp_ptrak,
         predicted_ufp_winsorized = ifelse(predicted_ufp < predicted_ufp_low, predicted_ufp_low,
                             ifelse(predicted_ufp > predicted_ufp_high, predicted_ufp_high, predicted_ufp)),
         
        predicted_ufp_winsorized_ptrak = ifelse(predicted_ufp_ptrak < predicted_ufp_low_ptrak, predicted_ufp_low_ptrak,
                             ifelse(predicted_ufp_ptrak > predicted_ufp_high_ptrak, predicted_ufp_high_ptrak, predicted_ufp_ptrak)),
         
         
         # adjustment based on the difference method
         diff_adjustment = predicted_lta_ufp - predicted_ufp,
         diff_adjustment_ptrak = predicted_lta_ufp_ptrak - predicted_ufp_ptrak,
         diff_adjustment_winsorize = predicted_lta_ufp - predicted_ufp_winsorized,
         diff_adjustment_winsorize_ptrak = predicted_lta_ufp_ptrak - predicted_ufp_winsorized_ptrak,
         )

saveRDS(nox, file.path("data", "epa_data_mart", "wa_county_nox_temp_adjustment.rda"))

# # QC - compare UFP predictions
# alt_r <- cor(nox$predicted_ufp, nox$predicted_ufp_alt) %>% round(2) 
# 
# print("predicted hourly UFP at synthetic site")
# nox %>%
#   ggplot(aes(x=predicted_ufp, y=predicted_ufp_alt)) + 
#   geom_point(alpha=0.1) + 
#   geom_smooth() + 
#   geom_abline(slope = 1, intercept = 0) + 
#   geom_abline(slope = c(0.8, 1.2), intercept = 0, linetype=2, col="red") + 
#   geom_text(aes(x=5e3, y=30e3, label=paste("R=", alt_r))) +
#   theme(aspect.ratio = 1) +
#   labs(title = "Predicted hourly UFP\nfrom a model with and without a DOW main effect", 
#        subtitle = "dashed lines are +-20%",
#        x = "Current (no DOW main effect)",
#        y = "Alternative: With DOW main effect"
#        )
# 
# ggsave(file.path(image_path, "..", "other", "qc", "predicted_ufp_dow_main_effect_comparison.png"), width = 6, height = 6)

```


```{r}
print("distribution of predicted hourly UFP")
summary(nox$predicted_ufp)

nox %>%
  mutate(predicted_ufp = predicted_ufp/ufp_conversion) %>%
  pivot_longer(cols = c(predicted_ufp, no2),names_to = "pollutant") %>%
  
  ggplot(data=, aes(x=time, y=value, col=pollutant)) + 
  geom_point(alpha=0.05) + 
  geom_smooth() + 
  
  geom_point(data=overnight %>% mutate(time = paste(date, hour),
                                       time = ymd_h(time,  tz=local_tz),
                                       pollutant = "ufp_observation",
                                       value = value/ufp_conversion), 
             alpha=0.3) +
  labs(title = paste0("observed hourly NO2 & UFP along with predicted  UFP (", ufp_conversion, " pt/cm3)"))

```

visualize fix no2 to the mean and see temporal patterns
* using long-term no2 data (dow, hour) with mean no2 observed

```{r}
test <- nox %>%
  mutate(hour = as.numeric(as.character(hour)),
         no2 = mean(no2, na.rm = T)) %>% 
  predict(lm1, 
          newdata=.) %>% 
  exp()

nox %>%
  mutate(predicted_ufp = test) %>%
  select(month, dow, hour, predicted_ufp) %>% 
  pivot_longer(cols = c(#month,
                        dow, hour)) %>%
  mutate(value = factor(value, levels = c(#levels(nox$month),
                                          levels(nox$dow), levels(nox$hour)))) %>%

  ggplot(aes(x=value, y=predicted_ufp)) +
  facet_wrap(~name, scales="free_x") +
  geom_boxplot() +
  labs(title = "predicted hourly UFP when NO2 is fixed", 
       subtitle = "NO2 is fixed to the mean NO2 observed during the study period")


nox %>%
  mutate(predicted_ufp = test) %>%
  select(month, dow, hour, predicted_ufp) %>% 
  ggplot(aes(x=hour, y=predicted_ufp)) +
  facet_wrap(~dow) +
  geom_point() +
    labs(title = "predicted hourly UFP when NO2 is fixed", 
         subtitle = "NO2 is fixed to the mean NO2 observed during the study period")
  
```

## temporal adjustment

```{r}
unbalanced_samples <- unbalanced_samples0 %>%
  #remove minutes & seconds
  mutate(time = ymd_h(substr(time, 1, 13), tz = local_tz)) %>%
  left_join(select(nox, time, contains("diff_adjustment"))) %>%
  mutate(ns_total_conc_adj_no2lm = ns_total_conc + diff_adjustment,
         ns_total_conc_adj_no2lm_winsorize = ns_total_conc + diff_adjustment_winsorize,
         
         ptrak_adj_no2lm = pnc_noscreen + diff_adjustment_ptrak,
         ptrak_adj_no2lm_winsorize = pnc_noscreen + diff_adjustment_winsorize_ptrak)
  

saveRDS(unbalanced_samples, file.path(dt_path, "campaign visit samples", "bh_visits_fixed_site_temp_adj.rds")) 

# prop.table(table(unbalanced_samples$ns_total_conc_adj_no2lm<=0))
# prop.table(table(is.na(unbalanced_samples$ns_total_conc_adj_no2lm)))

print("distribution of adjustments to visit-level concentrations")
summary(unbalanced_samples$diff_adjustment)
hist(unbalanced_samples$diff_adjustment)

summary(unbalanced_samples$diff_adjustment_winsorize)
hist(unbalanced_samples$diff_adjustment_winsorize)

# annual averages
adjusted_annual_avg <- unbalanced_samples %>% 
  group_by(design, version, campaign, location) %>%
  summarize_at(vars(contains(c( "ns_", "pnc_noscreen", "ptrak"))), ~mean(., na.rm = T)) %>%
  ungroup()


print("a handful of adjusted (non-winsorized) visit adjustments become very negative")
# comparison of stop-level adjustments
unbalanced_samples %>%
  pivot_longer(cols = contains("ns_total_conc_adj")) %>%  
  ggplot(aes(x=ns_total_conc, y=value, col=name)) + 
  facet_wrap(~design+version) +
  geom_point(alpha=0.01) + 
  geom_hline(yintercept = 0, col="gray") +
  geom_abline(slope = 1, intercept = 0, linetype=2) +
  geom_abline(slope = c(0.75, 1.25), intercept = 0, linetype=3, alpha=0.5) +
  geom_smooth() +
  labs(title = "unadjusted & adjusted visit-level samples",
       y = "adjusted ns_total_conc"
       )

```

```{r}
dt1 <- adjusted_annual_avg %>%
  select(design, version, campaign, location, ns_total_conc, pnc_noscreen)

dt2 <- adjusted_annual_avg %>%
  select(design, version, campaign, location, ns_total_conc = ns_total_conc_adj_no2lm_winsorize,
                                              pnc_noscreen = ptrak_adj_no2lm_winsorize) %>%
  mutate(version = paste(version, "temp adj 1"))


```



## comparison of annual averages

adjustments generally decrease annual average site estimates 

```{r}
#overnight_bh <- mean(overnight$value)

# adjusted_annual_avg %>%
#   left_join(all_data_annual) %>%
#   pivot_longer(cols = c(contains(c("ns_")))) %>%
#   ggplot(aes(x=version, y=value, col=name)) +
#     facet_wrap(~design, scales="free") +
#   # overnight BH estimate
#       geom_boxplot() +
#   labs(title = "Comparison of UFP annual site averages before/after temporal adjustment")
    

# # --> compare to all_data_annual? # left_join(all_data_annual) 
# adjusted_annual_avg %>%
#   pivot_longer(cols = contains("ns_total_conc_adj")) %>% 
#   ggplot(aes(x=ns_total_conc, y=value, col=name)) +
#   facet_wrap(~version) +
#   geom_point(alpha=0.1) +
#   geom_hline(yintercept = 0, col="gray") +
#   geom_abline(slope = 1, intercept = 0, linetype=2, alpha=0.5) +
#   geom_abline(slope = c(0.75, 1.25), intercept = 0, linetype=3, alpha=0.5) +
#   geom_smooth() + #stat_poly_eq() +
#   labs(title = "Comparison of NanoScan annual site averages before and after temporal adjustment",
#        subtitle = "dashed lines are the 1-1 and +-25%")


print("average/overall annual average site estimate correlations for unadjusted and adjusted data campaigns")

r <- lapply(unique(adjusted_annual_avg$version), function(x) {
  adjusted_annual_avg %>%
    left_join(all_data_annual) %>% 
    filter(version==x) %>%
    select(contains(c("ns_total_conc", "pnc_noscreen", "ptrak"))) %>%
    cor() 
  }) 

names(r) <- unique(adjusted_annual_avg$version)

r
  
```

# compare Ptrak vs NS hourly adjustments

```{r}
print("very high correlation between winsorized hourly adjustments from p-trak and ns readings")
cor(nox$diff_adjustment_winsorize, nox$diff_adjustment_winsorize_ptrak)

nox %>%
  ggplot(aes(x=diff_adjustment_winsorize, y=diff_adjustment_winsorize_ptrak)) + 
  geom_abline(slope = 1, intercept = 0, linetype=2) + 
  geom_point(alpha=0.3) + 
  geom_smooth() 
  
```



# ADJUSTMENT 2 - DON'T DO B/C BASED ON NS DATA & EXTREM ADJUSTMENTS ARE LARGER W/ NS (VS PTRAK DATA)?

```{r}
# OLD??

# background adjustment based on the collected data
# low_conc <- 0.01 
# # x = group_split(visits, adjusted, campaign, design, visits, cluster_type)[[1]]
# visits_adj2 <- lapply(group_split(unbalanced_samples0, design, version, campaign), function(x){
#   temp <- x %>%
#     mutate(hour = as.numeric(as.character(hour))) %>%
#     group_by(hour) %>%
#     mutate(quantile_background = quantile(ns_total_conc, low_conc)) %>% 
#     ungroup()
#   
#   # make the hourly adjustments smoother
#   temp_hrs <- distinct(temp, hour, quantile_background)
#   smooth_fit <- lm(quantile_background ~ ns(hour, knots=c(12, 15)), data = temp_hrs)
#   temp_hrs <- temp_hrs %>%
#     mutate(smooth_background = predict(smooth_fit, newdata=.),
#            # long-term average estimate based on background concentrations from the collected campaign data
#            lta_background = mean(smooth_background))
#      
#   # apply hourly adjustments, add the LTA background back in
#   temp <- left_join(temp, temp_hrs, by=c("hour", "quantile_background")) %>%
#     mutate(local_conc = ns_total_conc - smooth_background,
#            ns_total_conc_adjusted = local_conc + lta_background)
#   }) %>%
#   bind_rows() %>%
#   mutate(version = paste(version, "temp adj 2"))
# 
# saveRDS(visits_adj2, file.path(dt_path, "campaign visit samples", "bh_visits_background_temp_adj.rds")) 
# 
# annual_adj2 <- visits_adj2 %>%
#   group_by(design, version, campaign, location) %>%
#   summarize(ns_total_conc = mean(ns_total_conc_adjusted, na.rm=T)) %>%
#   ungroup() 

```

```{r}
# # dt2
# samples_temp_adj2 <- unbalanced_samples0 %>%
#   #remove minutes & seconds
#   mutate(time = ymd_h(substr(time, 1, 13), tz = local_tz)) %>%  
#   left_join(temp_adj2#, by="time"
#             ) %>% View()
#   mutate(version = paste(version, "temp adj 2"),
#          ns_total_conc_adj2 = ns_total_conc + avg_hourly_adj,
#          pnc_noscreen_adj2 = pnc_noscreen + avg_hourly_adj)
# 
# select(c(names(dt2), pnc_noscreen))  

```

```{r}
# --> ? compare site annual averages for: unadjusted, temp adj 1, temp adj 2



```



# SAVE DATA

```{r}
# save data
rbind(dt1, dt2) %>% 
  #rbind(annual_adj2) %>%
                      # output, v3_____    
  saveRDS(., file.path(dt_path, "temporal_adjustment.rda"))

```



# Appendix

### no2 model fit

how good is an no2 model fit (vs UFP)? 

not good when the model is similar (R2~ 0.09), and only slightly better when month is added 

MP: there is a lot of noise at any given time. NO2 helps us better capture otherwise unstructured concentration patterns

```{r}
print("similar model used to generate temporal adjustments")
no2_lm <- nox %>%
  mutate(hour = as.numeric(as.character(hour))) %>%
  lm(formula=log(no2)~dow*bs(hour, knots = c(5, 11, 17)),)

# R2~0.089
summary(no2_lm)


print("more specific model adding month")
# R2 ~ 0.182
no2_lm2 <- nox %>%
  mutate(hour = as.numeric(as.character(hour))) %>%
  lm(formula=log(no2)~dow*bs(hour, knots = c(5, 11, 17)) + month) 

summary(no2_lm2)


```





